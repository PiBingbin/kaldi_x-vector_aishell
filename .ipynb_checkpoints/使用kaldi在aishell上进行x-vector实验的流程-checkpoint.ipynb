{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在Aishell的数据库使用x-vector的方法训练说话人识别模型\n",
    "    \n",
    "    VAD，MFCC特征\n",
    "    采用数据增强手段 ，加入噪音，模拟混响\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第一步：** 准备数据配置索引文件\n",
    "\n",
    "    使用脚本生成下面的索引文件\n",
    "    \n",
    "    data\n",
    "    ├── test\n",
    "    │   ├── spk2utt\n",
    "    │   ├── text\n",
    "    │   ├── utt2spk\n",
    "    │   └── wav.scp\n",
    "    └── train\n",
    "        ├── spk2utt\n",
    "        ├── text\n",
    "        ├── utt2spk\n",
    "        └── wav.scp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data/local/train transcriptions\n",
      "Preparing data/local/test transcriptions\n",
      "local/aishell_data_prep.sh: AISHELL data preparation succeeded\n"
     ]
    }
   ],
   "source": [
    "!chmod +x new_01_prepare_train_index.sh\n",
    "!./new_01_prepare_train_index.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mdata\u001b[00m\r\n",
      "├── \u001b[01;34mlocal\u001b[00m\r\n",
      "│   ├── \u001b[01;34mdev\u001b[00m\r\n",
      "│   │   └── wav.flist\r\n",
      "│   ├── \u001b[01;34mtest\u001b[00m\r\n",
      "│   │   ├── spk2utt\r\n",
      "│   │   ├── text\r\n",
      "│   │   ├── transcripts.txt\r\n",
      "│   │   ├── utt2spk\r\n",
      "│   │   ├── utt2spk_all\r\n",
      "│   │   ├── utt.list\r\n",
      "│   │   ├── wav.flist\r\n",
      "│   │   ├── wav.scp\r\n",
      "│   │   └── wav.scp_all\r\n",
      "│   └── \u001b[01;34mtrain\u001b[00m\r\n",
      "│       ├── spk2utt\r\n",
      "│       ├── text\r\n",
      "│       ├── transcripts.txt\r\n",
      "│       ├── utt2spk\r\n",
      "│       ├── utt2spk_all\r\n",
      "│       ├── utt.list\r\n",
      "│       ├── wav.flist\r\n",
      "│       ├── wav.scp\r\n",
      "│       └── wav.scp_all\r\n",
      "├── \u001b[01;34mtest\u001b[00m\r\n",
      "│   ├── spk2utt\r\n",
      "│   ├── text\r\n",
      "│   ├── utt2spk\r\n",
      "│   └── wav.scp\r\n",
      "└── \u001b[01;34mtrain\u001b[00m\r\n",
      "    ├── spk2utt\r\n",
      "    ├── text\r\n",
      "    ├── utt2spk\r\n",
      "    └── wav.scp\r\n",
      "\r\n",
      "6 directories, 27 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第二步：** 准备原始语音的mfcc表征，并进行VAD操作\n",
    "\n",
    "    mfcc的参数：\n",
    "    --sample-frequency=16000 \n",
    "    --frame-length=25 # the default is 25\n",
    "    --low-freq=20 # the default.\n",
    "    --high-freq=8000 # the default is zero meaning use the Nyquist (4k in this case).\n",
    "    --num-ceps=23 # higher than the default which is 12.\n",
    "    --snip-edges=false\n",
    "    vad的参数：\n",
    "    --vad-energy-threshold=5.5\n",
    "    --vad-energy-mean-scale=0.5\n",
    "    --vad-proportion-threshold=0.12\n",
    "    --vad-frames-context=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps/make_mfcc.sh --write-utt2num-frames true --mfcc-config conf/mfcc.conf --nj 20 --cmd run.pl --mem 32G data/train exp/make_mfcc mfcc\n",
      "utils/validate_data_dir.sh: Successfully validated data-directory data/train\n",
      "steps/make_mfcc.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.\n",
      "steps/make_mfcc.sh: Succeeded creating MFCC features for train\n",
      "fix_data_dir.sh: kept all 120098 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/train/.backup\n",
      "sid/compute_vad_decision.sh --nj 40 --cmd run.pl --mem 32G data/train exp/make_vad vad\n",
      "Created VAD output for train\n",
      "fix_data_dir.sh: kept all 120098 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/train/.backup\n",
      "steps/make_mfcc.sh --write-utt2num-frames true --mfcc-config conf/mfcc.conf --nj 20 --cmd run.pl --mem 32G data/test exp/make_mfcc mfcc\n",
      "utils/validate_data_dir.sh: Successfully validated data-directory data/test\n",
      "steps/make_mfcc.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.\n",
      "steps/make_mfcc.sh: Succeeded creating MFCC features for test\n",
      "fix_data_dir.sh: kept all 7176 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/test/.backup\n",
      "sid/compute_vad_decision.sh --nj 40 --cmd run.pl --mem 32G data/test exp/make_vad vad\n",
      "utils/split_scp.pl: Refusing to split data because number of speakers 20 is less than the number of output .scp files 40\n",
      "fix_data_dir.sh: kept all 7176 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/test/.backup\n"
     ]
    }
   ],
   "source": [
    "!chmod +x new_02_generate_mfcc_and_vad.sh\n",
    "!./new_02_generate_mfcc_and_vad.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第三步：** 生成带有混响和噪音的索引文件\n",
    "\n",
    "    1. 按照1：1的数量，生成带有混响的语音\n",
    "    2. 按照1：3的数量，生成带有各种噪音的语音\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps/data/reverberate_data_dir.py:8: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import argparse, shlex, glob, math, os, random, sys, warnings, copy, imp, ast\n",
      "steps/data/reverberate_data_dir.py --rir-set-parameters 0.5, /home/qinyc/openSLR/SLR28_rir/RIRS_NOISES/simulated_rirs/smallroom/rir_list --rir-set-parameters 0.5, /home/qinyc/openSLR/SLR28_rir/RIRS_NOISES/simulated_rirs/mediumroom/rir_list --speech-rvb-probability 1 --pointsource-noise-addition-probability 0 --isotropic-noise-addition-probability 0 --num-replications 1 --source-sampling-rate 16000 data/train data/train_reverb\n",
      "Number of RIRs is 40000\n",
      "Getting the duration of the recordings...\n",
      "utils/copy_data_dir.sh: copied data from data/train_reverb to data/train_reverb.new\n",
      "utils/validate_data_dir.sh: Successfully validated data-directory data/train_reverb.new\n",
      "steps/data/make_musan.sh --sampling-rate 16000 /home/qinyc/openSLR/SLR17_musan/musan data\n",
      "steps/data/make_musan.py --use-vocals true --sampling-rate 16000 /home/qinyc/openSLR/SLR17_musan/musan data/musan\n",
      "Preparing data/musan/musan...\n",
      "In music directory, processed 645 files; 0 had missing wav data\n",
      "In speech directory, processed 426 files; 0 had missing wav data\n",
      "In noise directory, processed 930 files; 0 had missing wav data\n",
      "utils/fix_data_dir.sh: file data/musan/utt2spk is not in sorted order or not unique, sorting it\n",
      "utils/fix_data_dir.sh: file data/musan/wav.scp is not in sorted order or not unique, sorting it\n",
      "fix_data_dir.sh: kept all 2001 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/musan/.backup\n",
      "utils/subset_data_dir.sh: reducing #utt from 2001 to 645\n",
      "utils/subset_data_dir.sh: reducing #utt from 2001 to 426\n",
      "utils/subset_data_dir.sh: reducing #utt from 2001 to 930\n",
      "fix_data_dir.sh: kept all 645 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/musan_music/.backup\n",
      "fix_data_dir.sh: kept all 426 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/musan_speech/.backup\n",
      "fix_data_dir.sh: kept all 930 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/musan_noise/.backup\n",
      "utils/data/get_reco2dur.sh: obtaining durations from recordings\n",
      "utils/data/get_reco2dur.sh: could not get recording lengths from sphere-file headers, using wav-to-duration\n",
      "utils/data/get_reco2dur.sh: computed data/musan_speech/reco2dur\n",
      "utils/data/get_reco2dur.sh: obtaining durations from recordings\n",
      "cat: write error: Broken pipe\n",
      "utils/data/get_reco2dur.sh: could not get recording lengths from sphere-file headers, using wav-to-duration\n",
      "utils/data/get_reco2dur.sh: computed data/musan_noise/reco2dur\n",
      "utils/data/get_reco2dur.sh: obtaining durations from recordings\n",
      "utils/data/get_reco2dur.sh: could not get recording lengths from sphere-file headers, using wav-to-duration\n",
      "utils/data/get_reco2dur.sh: computed data/musan_music/reco2dur\n",
      "utils/data/get_utt2dur.sh: segments file does not exist so getting durations from wave files\n",
      "utils/data/get_utt2dur.sh: could not get utterance lengths from sphere-file headers, using wav-to-duration\n",
      "utils/data/get_utt2dur.sh: computed data/musan_speech/utt2dur\n",
      "utils/data/get_utt2dur.sh: segments file does not exist so getting durations from wave files\n",
      "utils/data/get_utt2dur.sh: could not get utterance lengths from sphere-file headers, using wav-to-duration\n",
      "utils/data/get_utt2dur.sh: computed data/musan_noise/utt2dur\n",
      "utils/data/get_utt2dur.sh: segments file does not exist so getting durations from wave files\n",
      "utils/data/get_utt2dur.sh: could not get utterance lengths from sphere-file headers, using wav-to-duration\n",
      "utils/data/get_utt2dur.sh: computed data/musan_music/utt2dur\n",
      "steps/data/augment_data_dir.py:12: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import sys, random, argparse, os, imp\n",
      "steps/data/augment_data_dir.py --utt-suffix noise --fg-interval 1 --fg-snrs 15:10:5:0 --fg-noise-dir data/musan_noise data/train data/train_noise\n",
      "steps/data/augment_data_dir.py:12: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import sys, random, argparse, os, imp\n",
      "steps/data/augment_data_dir.py --utt-suffix music --bg-snrs 15:10:8:5 --num-bg-noises 1 --bg-noise-dir data/musan_music data/train data/train_music\n",
      "steps/data/augment_data_dir.py:12: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import sys, random, argparse, os, imp\n",
      "steps/data/augment_data_dir.py --utt-suffix babble --bg-snrs 20:17:15:13 --num-bg-noises 3:4:5:6:7 --bg-noise-dir data/musan_speech data/train data/train_babble\n",
      "utils/combine_data.sh data/train_aug data/train_reverb data/train_noise data/train_music data/train_babble\n",
      "utils/combine_data.sh: combined utt2uniq\n",
      "utils/combine_data.sh [info]: not combining segments as it does not exist\n",
      "utils/combine_data.sh: combined utt2spk\n",
      "utils/combine_data.sh [info]: not combining utt2lang as it does not exist\n",
      "utils/combine_data.sh [info]: **not combining utt2dur as it does not exist everywhere**\n",
      "utils/combine_data.sh [info]: **not combining utt2num_frames as it does not exist everywhere**\n",
      "utils/combine_data.sh [info]: **not combining reco2dur as it does not exist everywhere**\n",
      "utils/combine_data.sh [info]: not combining feats.scp as it does not exist\n",
      "utils/combine_data.sh: combined text\n",
      "utils/combine_data.sh [info]: not combining cmvn.scp as it does not exist\n",
      "utils/combine_data.sh: combined vad.scp\n",
      "utils/combine_data.sh [info]: not combining reco2file_and_channel as it does not exist\n",
      "utils/combine_data.sh: combined wav.scp\n",
      "utils/combine_data.sh [info]: not combining spk2gender as it does not exist\n",
      "fix_data_dir.sh: kept all 480392 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/train_aug/.backup\n"
     ]
    }
   ],
   "source": [
    "!chmod +x new_03_prepare_dataaugment.sh\n",
    "!./new_03_prepare_dataaugment.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第四步：** 将第三步中扩充的数据提取为MFCC表征\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps/make_mfcc.sh --mfcc-config conf/mfcc.conf --nj 20 --cmd run.pl --mem 32G data/train_aug exp/make_mfcc mfcc_aug\n",
      "utils/validate_data_dir.sh: Successfully validated data-directory data/train_aug\n",
      "steps/make_mfcc.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.\n",
      "steps/make_mfcc.sh: Succeeded creating MFCC features for train_aug\n"
     ]
    }
   ],
   "source": [
    "!chmod +x new_04_extract_feature.sh\n",
    "!./new_04_extract_feature.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第五步：** 过滤语音\n",
    "    \n",
    "    ## 1. 将train_aug数据和train数据合并\n",
    "    ## 2. 该脚本应用CMVN并删除非语音帧。 会重新复制一份feature，确保磁盘空间。\n",
    "    ## 3. 删除静音帧后删除过短的功能。\n",
    "    ## 4. 去除语音数量少于8个的说话人。\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils/combine_data.sh data/train_combined data/train_aug data/train\n",
      "utils/combine_data.sh: combined utt2uniq\n",
      "utils/combine_data.sh [info]: not combining segments as it does not exist\n",
      "utils/combine_data.sh: combined utt2spk\n",
      "utils/combine_data.sh [info]: not combining utt2lang as it does not exist\n",
      "utils/combine_data.sh: combined utt2dur\n",
      "utils/combine_data.sh: combined utt2num_frames\n",
      "utils/combine_data.sh [info]: **not combining reco2dur as it does not exist everywhere**\n",
      "utils/combine_data.sh: combined feats.scp\n",
      "utils/combine_data.sh: combined text\n",
      "utils/combine_data.sh [info]: not combining cmvn.scp as it does not exist\n",
      "utils/combine_data.sh: combined vad.scp\n",
      "utils/combine_data.sh [info]: not combining reco2file_and_channel as it does not exist\n",
      "utils/combine_data.sh: combined wav.scp\n",
      "utils/combine_data.sh [info]: not combining spk2gender as it does not exist\n",
      "fix_data_dir.sh: kept all 600490 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/train_combined/.backup\n",
      "fix_data_dir.sh: kept all 600490 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/train_combined/.backup\n",
      "local/nnet3/xvector/prepare_feats_for_egs.sh --nj 40 --cmd run.pl --mem 32G data/train_combined data/train_combined_no_sil exp/train_combined_no_sil\n",
      "local/nnet3/xvector/prepare_feats_for_egs.sh: Succeeded creating xvector features for train_combined\n",
      "fix_data_dir.sh: kept all 600490 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/train_combined_no_sil/.backup\n",
      "fix_data_dir.sh: kept all 590655 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/train_combined_no_sil/.backup\n",
      "fix_data_dir.sh: kept all 590655 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/train_combined_no_sil/.backup\n"
     ]
    }
   ],
   "source": [
    "!chmod +x new_05_filter_feature.sh\n",
    "!./new_05_filter_feature.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第六步：** 建立训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_06_train_x-vector.sh: creating neural net configs using the xconfig parser\n",
      "steps/nnet3/xconfig_to_configs.py --xconfig-file exp/xvector_nnet_1a/configs/network.xconfig --config-dir exp/xvector_nnet_1a/configs/\n",
      "nnet3-init exp/xvector_nnet_1a/configs//ref.config exp/xvector_nnet_1a/configs//ref.raw \n",
      "LOG (nnet3-init[5.5.480~1-09abd]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/xvector_nnet_1a/configs//ref.raw\n",
      "nnet3-info exp/xvector_nnet_1a/configs//ref.raw \n",
      "nnet3-init exp/xvector_nnet_1a/configs//ref.config exp/xvector_nnet_1a/configs//ref.raw \n",
      "LOG (nnet3-init[5.5.480~1-09abd]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/xvector_nnet_1a/configs//ref.raw\n",
      "nnet3-info exp/xvector_nnet_1a/configs//ref.raw \n",
      "2020-11-02 17:19:09,988 [steps/nnet3/train_raw_dnn.py:35 - <module> - INFO ] Starting raw DNN trainer (train_raw_dnn.py)\n",
      "steps/nnet3/train_raw_dnn.py --stage=0 --cmd=run.pl --mem 10G --trainer.optimization.proportional-shrink 10 --trainer.optimization.momentum=0.5 --trainer.optimization.num-jobs-initial=1 --trainer.optimization.num-jobs-final=1 --trainer.optimization.initial-effective-lrate=0.001 --trainer.optimization.final-effective-lrate=0.0001 --trainer.optimization.minibatch-size=64 --trainer.srand=123 --trainer.max-param-change=2 --trainer.num-epochs=20 --trainer.dropout-schedule=0,0@0.20,0.1@0.50,0 --trainer.shuffle-buffer-size=1000 --egs.frames-per-eg=1 --egs.dir=exp/xvector_nnet_1a/egs --cleanup.remove-egs false --cleanup.preserve-model-interval=10 --use-gpu=true --dir=exp/xvector_nnet_1a\n",
      "['steps/nnet3/train_raw_dnn.py', '--stage=0', '--cmd=run.pl --mem 10G', '--trainer.optimization.proportional-shrink', '10', '--trainer.optimization.momentum=0.5', '--trainer.optimization.num-jobs-initial=1', '--trainer.optimization.num-jobs-final=1', '--trainer.optimization.initial-effective-lrate=0.001', '--trainer.optimization.final-effective-lrate=0.0001', '--trainer.optimization.minibatch-size=64', '--trainer.srand=123', '--trainer.max-param-change=2', '--trainer.num-epochs=20', '--trainer.dropout-schedule=0,0@0.20,0.1@0.50,0', '--trainer.shuffle-buffer-size=1000', '--egs.frames-per-eg=1', '--egs.dir=exp/xvector_nnet_1a/egs', '--cleanup.remove-egs', 'false', '--cleanup.preserve-model-interval=10', '--use-gpu=true', '--dir=exp/xvector_nnet_1a']\n",
      "2020-11-02 17:19:09,992 [steps/nnet3/train_raw_dnn.py:196 - train - INFO ] Arguments for the experiment\n",
      "{'backstitch_training_interval': 1,\n",
      " 'backstitch_training_scale': 0.0,\n",
      " 'cleanup': True,\n",
      " 'cmvn_opts': None,\n",
      " 'combine_sum_to_one_penalty': 0.0,\n",
      " 'command': 'run.pl --mem 10G',\n",
      " 'compute_average_posteriors': False,\n",
      " 'compute_per_dim_accuracy': False,\n",
      " 'dir': 'exp/xvector_nnet_1a',\n",
      " 'do_final_combination': True,\n",
      " 'dropout_schedule': '0,0@0.20,0.1@0.50,0',\n",
      " 'egs_command': None,\n",
      " 'egs_dir': 'exp/xvector_nnet_1a/egs',\n",
      " 'egs_opts': None,\n",
      " 'egs_stage': 0,\n",
      " 'email': None,\n",
      " 'exit_stage': None,\n",
      " 'feat_dir': None,\n",
      " 'final_effective_lrate': 0.0001,\n",
      " 'frames_per_eg': 1,\n",
      " 'image_augmentation_opts': None,\n",
      " 'initial_effective_lrate': 0.001,\n",
      " 'input_model': None,\n",
      " 'max_lda_jobs': 10,\n",
      " 'max_models_combine': 20,\n",
      " 'max_objective_evaluations': 30,\n",
      " 'max_param_change': 2.0,\n",
      " 'minibatch_size': '64',\n",
      " 'momentum': 0.5,\n",
      " 'nj': 4,\n",
      " 'num_epochs': 20.0,\n",
      " 'num_jobs_compute_prior': 10,\n",
      " 'num_jobs_final': 1,\n",
      " 'num_jobs_initial': 1,\n",
      " 'num_jobs_step': 1,\n",
      " 'online_ivector_dir': None,\n",
      " 'preserve_model_interval': 10,\n",
      " 'presoftmax_prior_scale_power': -0.25,\n",
      " 'prior_subset_size': 20000,\n",
      " 'proportional_shrink': 10.0,\n",
      " 'rand_prune': 4.0,\n",
      " 'remove_egs': False,\n",
      " 'reporting_interval': 0.1,\n",
      " 'samples_per_iter': 400000,\n",
      " 'shuffle_buffer_size': 1000,\n",
      " 'srand': 123,\n",
      " 'stage': 0,\n",
      " 'targets_scp': None,\n",
      " 'train_opts': [],\n",
      " 'use_dense_targets': True,\n",
      " 'use_gpu': 'yes'}\n",
      "2020-11-02 17:19:09,993 [steps/nnet3/train_raw_dnn.py:354 - train - INFO ] Training will run for 20.0 epochs = 180 iterations\n",
      "2020-11-02 17:19:09,993 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 0/179   Jobs: 1   Epoch: 0.00/20.0 (0.0% complete)   lr: 0.001000   shrink: 0.99000\n",
      "2020-11-02 17:19:17,990 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 1/179   Jobs: 1   Epoch: 0.11/20.0 (0.6% complete)   lr: 0.000987   shrink: 0.99013\n",
      "2020-11-02 17:19:26,026 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 2/179   Jobs: 1   Epoch: 0.22/20.0 (1.1% complete)   lr: 0.000975   shrink: 0.99025\n",
      "2020-11-02 17:19:32,968 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 3/179   Jobs: 1   Epoch: 0.33/20.0 (1.7% complete)   lr: 0.000962   shrink: 0.99038\n",
      "2020-11-02 17:19:39,931 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 4/179   Jobs: 1   Epoch: 0.44/20.0 (2.2% complete)   lr: 0.000950   shrink: 0.99050\n",
      "2020-11-02 17:19:48,819 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 5/179   Jobs: 1   Epoch: 0.56/20.0 (2.8% complete)   lr: 0.000938   shrink: 0.99062\n",
      "2020-11-02 17:19:56,732 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 6/179   Jobs: 1   Epoch: 0.67/20.0 (3.3% complete)   lr: 0.000926   shrink: 0.99074\n",
      "2020-11-02 17:20:07,498 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 7/179   Jobs: 1   Epoch: 0.78/20.0 (3.9% complete)   lr: 0.000914   shrink: 0.99086\n",
      "2020-11-02 17:20:16,882 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 8/179   Jobs: 1   Epoch: 0.89/20.0 (4.4% complete)   lr: 0.000903   shrink: 0.99097\n",
      "2020-11-02 17:20:27,726 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 9/179   Jobs: 1   Epoch: 1.00/20.0 (5.0% complete)   lr: 0.000891   shrink: 0.99109\n",
      "2020-11-02 17:20:34,363 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 10/179   Jobs: 1   Epoch: 1.11/20.0 (5.6% complete)   lr: 0.000880   shrink: 0.99120\n",
      "2020-11-02 17:20:41,997 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 11/179   Jobs: 1   Epoch: 1.22/20.0 (6.1% complete)   lr: 0.000869   shrink: 0.99131\n",
      "2020-11-02 17:20:48,377 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 12/179   Jobs: 1   Epoch: 1.33/20.0 (6.7% complete)   lr: 0.000858   shrink: 0.99142\n",
      "2020-11-02 17:20:54,849 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 13/179   Jobs: 1   Epoch: 1.44/20.0 (7.2% complete)   lr: 0.000847   shrink: 0.99153\n",
      "2020-11-02 17:21:03,336 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 14/179   Jobs: 1   Epoch: 1.56/20.0 (7.8% complete)   lr: 0.000836   shrink: 0.99164\n",
      "2020-11-02 17:21:10,806 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 15/179   Jobs: 1   Epoch: 1.67/20.0 (8.3% complete)   lr: 0.000825   shrink: 0.99175\n",
      "2020-11-02 17:21:21,228 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 16/179   Jobs: 1   Epoch: 1.78/20.0 (8.9% complete)   lr: 0.000815   shrink: 0.99185\n",
      "2020-11-02 17:21:30,168 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 17/179   Jobs: 1   Epoch: 1.89/20.0 (9.4% complete)   lr: 0.000805   shrink: 0.99195\n",
      "2020-11-02 17:21:40,656 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 18/179   Jobs: 1   Epoch: 2.00/20.0 (10.0% complete)   lr: 0.000794   shrink: 0.99206\n",
      "2020-11-02 17:21:47,424 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 19/179   Jobs: 1   Epoch: 2.11/20.0 (10.6% complete)   lr: 0.000784   shrink: 0.99216\n",
      "2020-11-02 17:21:55,209 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 20/179   Jobs: 1   Epoch: 2.22/20.0 (11.1% complete)   lr: 0.000774   shrink: 0.99226\n",
      "2020-11-02 17:22:02,501 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 21/179   Jobs: 1   Epoch: 2.33/20.0 (11.7% complete)   lr: 0.000764   shrink: 0.99236\n",
      "2020-11-02 17:22:09,086 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 22/179   Jobs: 1   Epoch: 2.44/20.0 (12.2% complete)   lr: 0.000755   shrink: 0.99245\n",
      "2020-11-02 17:22:17,530 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 23/179   Jobs: 1   Epoch: 2.56/20.0 (12.8% complete)   lr: 0.000745   shrink: 0.99255\n",
      "2020-11-02 17:22:24,990 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 24/179   Jobs: 1   Epoch: 2.67/20.0 (13.3% complete)   lr: 0.000736   shrink: 0.99264\n",
      "2020-11-02 17:22:35,435 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 25/179   Jobs: 1   Epoch: 2.78/20.0 (13.9% complete)   lr: 0.000726   shrink: 0.99274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 17:22:44,315 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 26/179   Jobs: 1   Epoch: 2.89/20.0 (14.4% complete)   lr: 0.000717   shrink: 0.99283\n",
      "2020-11-02 17:22:54,779 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 27/179   Jobs: 1   Epoch: 3.00/20.0 (15.0% complete)   lr: 0.000708   shrink: 0.99292\n",
      "2020-11-02 17:23:01,527 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 28/179   Jobs: 1   Epoch: 3.11/20.0 (15.6% complete)   lr: 0.000699   shrink: 0.99301\n",
      "2020-11-02 17:23:09,374 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 29/179   Jobs: 1   Epoch: 3.22/20.0 (16.1% complete)   lr: 0.000690   shrink: 0.99310\n",
      "2020-11-02 17:23:15,896 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 30/179   Jobs: 1   Epoch: 3.33/20.0 (16.7% complete)   lr: 0.000681   shrink: 0.99319\n",
      "2020-11-02 17:23:22,532 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 31/179   Jobs: 1   Epoch: 3.44/20.0 (17.2% complete)   lr: 0.000673   shrink: 0.99327\n",
      "2020-11-02 17:23:30,995 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 32/179   Jobs: 1   Epoch: 3.56/20.0 (17.8% complete)   lr: 0.000664   shrink: 0.99336\n",
      "2020-11-02 17:23:38,577 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 33/179   Jobs: 1   Epoch: 3.67/20.0 (18.3% complete)   lr: 0.000656   shrink: 0.99344\n",
      "2020-11-02 17:23:48,995 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 34/179   Jobs: 1   Epoch: 3.78/20.0 (18.9% complete)   lr: 0.000647   shrink: 0.99353\n",
      "2020-11-02 17:23:57,917 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 35/179   Jobs: 1   Epoch: 3.89/20.0 (19.4% complete)   lr: 0.000639   shrink: 0.99361\n",
      "2020-11-02 17:24:08,304 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 36/179   Jobs: 1   Epoch: 4.00/20.0 (20.0% complete)   lr: 0.000631   shrink: 0.99369\n",
      "2020-11-02 17:24:15,139 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 37/179   Jobs: 1   Epoch: 4.11/20.0 (20.6% complete)   lr: 0.000623   shrink: 0.99377\n",
      "2020-11-02 17:24:23,016 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 38/179   Jobs: 1   Epoch: 4.22/20.0 (21.1% complete)   lr: 0.000615   shrink: 0.99385\n",
      "2020-11-02 17:24:29,545 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 39/179   Jobs: 1   Epoch: 4.33/20.0 (21.7% complete)   lr: 0.000607   shrink: 0.99393\n",
      "2020-11-02 17:24:36,113 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 40/179   Jobs: 1   Epoch: 4.44/20.0 (22.2% complete)   lr: 0.000599   shrink: 0.99401\n",
      "2020-11-02 17:24:45,343 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 41/179   Jobs: 1   Epoch: 4.56/20.0 (22.8% complete)   lr: 0.000592   shrink: 0.99408\n",
      "2020-11-02 17:24:52,865 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 42/179   Jobs: 1   Epoch: 4.67/20.0 (23.3% complete)   lr: 0.000584   shrink: 0.99416\n",
      "2020-11-02 17:25:03,356 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 43/179   Jobs: 1   Epoch: 4.78/20.0 (23.9% complete)   lr: 0.000577   shrink: 0.99423\n",
      "2020-11-02 17:25:12,262 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 44/179   Jobs: 1   Epoch: 4.89/20.0 (24.4% complete)   lr: 0.000570   shrink: 0.99430\n",
      "2020-11-02 17:25:22,714 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 45/179   Jobs: 1   Epoch: 5.00/20.0 (25.0% complete)   lr: 0.000562   shrink: 0.99438\n",
      "2020-11-02 17:25:29,489 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 46/179   Jobs: 1   Epoch: 5.11/20.0 (25.6% complete)   lr: 0.000555   shrink: 0.99445\n",
      "2020-11-02 17:25:37,290 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 47/179   Jobs: 1   Epoch: 5.22/20.0 (26.1% complete)   lr: 0.000548   shrink: 0.99452\n",
      "2020-11-02 17:25:43,826 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 48/179   Jobs: 1   Epoch: 5.33/20.0 (26.7% complete)   lr: 0.000541   shrink: 0.99459\n",
      "2020-11-02 17:25:50,389 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 49/179   Jobs: 1   Epoch: 5.44/20.0 (27.2% complete)   lr: 0.000534   shrink: 0.99466\n",
      "2020-11-02 17:25:58,867 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 50/179   Jobs: 1   Epoch: 5.56/20.0 (27.8% complete)   lr: 0.000527   shrink: 0.99473\n",
      "2020-11-02 17:26:06,443 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 51/179   Jobs: 1   Epoch: 5.67/20.0 (28.3% complete)   lr: 0.000521   shrink: 0.99479\n",
      "2020-11-02 17:26:16,839 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 52/179   Jobs: 1   Epoch: 5.78/20.0 (28.9% complete)   lr: 0.000514   shrink: 0.99486\n",
      "2020-11-02 17:26:25,788 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 53/179   Jobs: 1   Epoch: 5.89/20.0 (29.4% complete)   lr: 0.000508   shrink: 0.99492\n",
      "2020-11-02 17:26:36,278 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 54/179   Jobs: 1   Epoch: 6.00/20.0 (30.0% complete)   lr: 0.000501   shrink: 0.99499\n",
      "2020-11-02 17:26:43,044 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 55/179   Jobs: 1   Epoch: 6.11/20.0 (30.6% complete)   lr: 0.000495   shrink: 0.99505\n",
      "2020-11-02 17:26:50,847 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 56/179   Jobs: 1   Epoch: 6.22/20.0 (31.1% complete)   lr: 0.000489   shrink: 0.99511\n",
      "2020-11-02 17:26:57,441 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 57/179   Jobs: 1   Epoch: 6.33/20.0 (31.7% complete)   lr: 0.000482   shrink: 0.99518\n",
      "2020-11-02 17:27:04,008 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 58/179   Jobs: 1   Epoch: 6.44/20.0 (32.2% complete)   lr: 0.000476   shrink: 0.99524\n",
      "2020-11-02 17:27:12,497 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 59/179   Jobs: 1   Epoch: 6.56/20.0 (32.8% complete)   lr: 0.000470   shrink: 0.99530\n",
      "2020-11-02 17:27:19,975 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 60/179   Jobs: 1   Epoch: 6.67/20.0 (33.3% complete)   lr: 0.000464   shrink: 0.99536\n",
      "2020-11-02 17:27:31,074 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 61/179   Jobs: 1   Epoch: 6.78/20.0 (33.9% complete)   lr: 0.000458   shrink: 0.99542\n",
      "2020-11-02 17:27:39,969 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 62/179   Jobs: 1   Epoch: 6.89/20.0 (34.4% complete)   lr: 0.000452   shrink: 0.99548\n",
      "2020-11-02 17:27:50,389 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 63/179   Jobs: 1   Epoch: 7.00/20.0 (35.0% complete)   lr: 0.000447   shrink: 0.99553\n",
      "2020-11-02 17:27:57,151 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 64/179   Jobs: 1   Epoch: 7.11/20.0 (35.6% complete)   lr: 0.000441   shrink: 0.99559\n",
      "2020-11-02 17:28:05,035 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 65/179   Jobs: 1   Epoch: 7.22/20.0 (36.1% complete)   lr: 0.000435   shrink: 0.99565\n",
      "2020-11-02 17:28:11,583 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 66/179   Jobs: 1   Epoch: 7.33/20.0 (36.7% complete)   lr: 0.000430   shrink: 0.99570\n",
      "2020-11-02 17:28:18,217 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 67/179   Jobs: 1   Epoch: 7.44/20.0 (37.2% complete)   lr: 0.000424   shrink: 0.99576\n",
      "2020-11-02 17:28:26,666 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 68/179   Jobs: 1   Epoch: 7.56/20.0 (37.8% complete)   lr: 0.000419   shrink: 0.99581\n",
      "2020-11-02 17:28:34,190 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 69/179   Jobs: 1   Epoch: 7.67/20.0 (38.3% complete)   lr: 0.000414   shrink: 0.99586\n",
      "2020-11-02 17:28:44,643 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 70/179   Jobs: 1   Epoch: 7.78/20.0 (38.9% complete)   lr: 0.000408   shrink: 0.99592\n",
      "2020-11-02 17:28:53,575 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 71/179   Jobs: 1   Epoch: 7.89/20.0 (39.4% complete)   lr: 0.000403   shrink: 0.99597\n",
      "2020-11-02 17:29:04,025 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 72/179   Jobs: 1   Epoch: 8.00/20.0 (40.0% complete)   lr: 0.000398   shrink: 0.99602\n",
      "2020-11-02 17:29:10,815 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 73/179   Jobs: 1   Epoch: 8.11/20.0 (40.6% complete)   lr: 0.000393   shrink: 0.99607\n",
      "2020-11-02 17:29:18,606 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 74/179   Jobs: 1   Epoch: 8.22/20.0 (41.1% complete)   lr: 0.000388   shrink: 0.99612\n",
      "2020-11-02 17:29:25,112 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 75/179   Jobs: 1   Epoch: 8.33/20.0 (41.7% complete)   lr: 0.000383   shrink: 0.99617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 17:29:31,693 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 76/179   Jobs: 1   Epoch: 8.44/20.0 (42.2% complete)   lr: 0.000378   shrink: 0.99622\n",
      "2020-11-02 17:29:40,200 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 77/179   Jobs: 1   Epoch: 8.56/20.0 (42.8% complete)   lr: 0.000373   shrink: 0.99627\n",
      "2020-11-02 17:29:47,725 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 78/179   Jobs: 1   Epoch: 8.67/20.0 (43.3% complete)   lr: 0.000369   shrink: 0.99631\n",
      "2020-11-02 17:29:58,173 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 79/179   Jobs: 1   Epoch: 8.78/20.0 (43.9% complete)   lr: 0.000364   shrink: 0.99636\n",
      "2020-11-02 17:30:07,004 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 80/179   Jobs: 1   Epoch: 8.89/20.0 (44.4% complete)   lr: 0.000359   shrink: 0.99641\n",
      "2020-11-02 17:30:18,129 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 81/179   Jobs: 1   Epoch: 9.00/20.0 (45.0% complete)   lr: 0.000355   shrink: 0.99645\n",
      "2020-11-02 17:30:24,930 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 82/179   Jobs: 1   Epoch: 9.11/20.0 (45.6% complete)   lr: 0.000350   shrink: 0.99650\n",
      "2020-11-02 17:30:32,741 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 83/179   Jobs: 1   Epoch: 9.22/20.0 (46.1% complete)   lr: 0.000346   shrink: 0.99654\n",
      "2020-11-02 17:30:39,250 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 84/179   Jobs: 1   Epoch: 9.33/20.0 (46.7% complete)   lr: 0.000341   shrink: 0.99659\n",
      "2020-11-02 17:30:45,849 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 85/179   Jobs: 1   Epoch: 9.44/20.0 (47.2% complete)   lr: 0.000337   shrink: 0.99663\n",
      "2020-11-02 17:30:54,325 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 86/179   Jobs: 1   Epoch: 9.56/20.0 (47.8% complete)   lr: 0.000333   shrink: 0.99667\n",
      "2020-11-02 17:31:01,826 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 87/179   Jobs: 1   Epoch: 9.67/20.0 (48.3% complete)   lr: 0.000329   shrink: 0.99671\n",
      "2020-11-02 17:31:12,309 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 88/179   Jobs: 1   Epoch: 9.78/20.0 (48.9% complete)   lr: 0.000324   shrink: 0.99676\n",
      "2020-11-02 17:31:21,267 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 89/179   Jobs: 1   Epoch: 9.89/20.0 (49.4% complete)   lr: 0.000320   shrink: 0.99680\n",
      "2020-11-02 17:31:31,724 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 90/179   Jobs: 1   Epoch: 10.00/20.0 (50.0% complete)   lr: 0.000316   shrink: 0.99684\n",
      "2020-11-02 17:31:38,610 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 91/179   Jobs: 1   Epoch: 10.11/20.0 (50.6% complete)   lr: 0.000312   shrink: 0.99688\n",
      "2020-11-02 17:31:46,405 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 92/179   Jobs: 1   Epoch: 10.22/20.0 (51.1% complete)   lr: 0.000308   shrink: 0.99692\n",
      "2020-11-02 17:31:53,012 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 93/179   Jobs: 1   Epoch: 10.33/20.0 (51.7% complete)   lr: 0.000304   shrink: 0.99696\n",
      "2020-11-02 17:31:59,577 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 94/179   Jobs: 1   Epoch: 10.44/20.0 (52.2% complete)   lr: 0.000300   shrink: 0.99700\n",
      "2020-11-02 17:32:08,136 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 95/179   Jobs: 1   Epoch: 10.56/20.0 (52.8% complete)   lr: 0.000297   shrink: 0.99703\n",
      "2020-11-02 17:32:15,674 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 96/179   Jobs: 1   Epoch: 10.67/20.0 (53.3% complete)   lr: 0.000293   shrink: 0.99707\n",
      "2020-11-02 17:32:26,136 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 97/179   Jobs: 1   Epoch: 10.78/20.0 (53.9% complete)   lr: 0.000289   shrink: 0.99711\n",
      "2020-11-02 17:32:35,044 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 98/179   Jobs: 1   Epoch: 10.89/20.0 (54.4% complete)   lr: 0.000285   shrink: 0.99715\n",
      "2020-11-02 17:32:45,477 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 99/179   Jobs: 1   Epoch: 11.00/20.0 (55.0% complete)   lr: 0.000282   shrink: 0.99718\n",
      "2020-11-02 17:32:52,239 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 100/179   Jobs: 1   Epoch: 11.11/20.0 (55.6% complete)   lr: 0.000278   shrink: 0.99722\n",
      "2020-11-02 17:33:00,792 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 101/179   Jobs: 1   Epoch: 11.22/20.0 (56.1% complete)   lr: 0.000275   shrink: 0.99725\n",
      "2020-11-02 17:33:07,354 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 102/179   Jobs: 1   Epoch: 11.33/20.0 (56.7% complete)   lr: 0.000271   shrink: 0.99729\n",
      "2020-11-02 17:33:14,034 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 103/179   Jobs: 1   Epoch: 11.44/20.0 (57.2% complete)   lr: 0.000268   shrink: 0.99732\n",
      "2020-11-02 17:33:22,508 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 104/179   Jobs: 1   Epoch: 11.56/20.0 (57.8% complete)   lr: 0.000264   shrink: 0.99736\n",
      "2020-11-02 17:33:30,059 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 105/179   Jobs: 1   Epoch: 11.67/20.0 (58.3% complete)   lr: 0.000261   shrink: 0.99739\n",
      "2020-11-02 17:33:40,564 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 106/179   Jobs: 1   Epoch: 11.78/20.0 (58.9% complete)   lr: 0.000258   shrink: 0.99742\n",
      "2020-11-02 17:33:49,495 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 107/179   Jobs: 1   Epoch: 11.89/20.0 (59.4% complete)   lr: 0.000254   shrink: 0.99746\n",
      "2020-11-02 17:33:59,970 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 108/179   Jobs: 1   Epoch: 12.00/20.0 (60.0% complete)   lr: 0.000251   shrink: 0.99749\n",
      "2020-11-02 17:34:06,776 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 109/179   Jobs: 1   Epoch: 12.11/20.0 (60.6% complete)   lr: 0.000248   shrink: 0.99752\n",
      "2020-11-02 17:34:14,589 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 110/179   Jobs: 1   Epoch: 12.22/20.0 (61.1% complete)   lr: 0.000245   shrink: 0.99755\n",
      "2020-11-02 17:34:21,168 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 111/179   Jobs: 1   Epoch: 12.33/20.0 (61.7% complete)   lr: 0.000242   shrink: 0.99758\n",
      "2020-11-02 17:34:27,782 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 112/179   Jobs: 1   Epoch: 12.44/20.0 (62.2% complete)   lr: 0.000239   shrink: 0.99761\n",
      "2020-11-02 17:34:36,281 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 113/179   Jobs: 1   Epoch: 12.56/20.0 (62.8% complete)   lr: 0.000236   shrink: 0.99764\n",
      "2020-11-02 17:34:43,774 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 114/179   Jobs: 1   Epoch: 12.67/20.0 (63.3% complete)   lr: 0.000233   shrink: 0.99767\n",
      "2020-11-02 17:34:54,250 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 115/179   Jobs: 1   Epoch: 12.78/20.0 (63.9% complete)   lr: 0.000230   shrink: 0.99770\n",
      "2020-11-02 17:35:03,149 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 116/179   Jobs: 1   Epoch: 12.89/20.0 (64.4% complete)   lr: 0.000227   shrink: 0.99773\n",
      "2020-11-02 17:35:13,621 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 117/179   Jobs: 1   Epoch: 13.00/20.0 (65.0% complete)   lr: 0.000224   shrink: 0.99776\n",
      "2020-11-02 17:35:20,422 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 118/179   Jobs: 1   Epoch: 13.11/20.0 (65.6% complete)   lr: 0.000221   shrink: 0.99779\n",
      "2020-11-02 17:35:28,238 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 119/179   Jobs: 1   Epoch: 13.22/20.0 (66.1% complete)   lr: 0.000218   shrink: 0.99782\n",
      "2020-11-02 17:35:34,716 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 120/179   Jobs: 1   Epoch: 13.33/20.0 (66.7% complete)   lr: 0.000215   shrink: 0.99785\n",
      "2020-11-02 17:35:42,069 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 121/179   Jobs: 1   Epoch: 13.44/20.0 (67.2% complete)   lr: 0.000213   shrink: 0.99787\n",
      "2020-11-02 17:35:50,576 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 122/179   Jobs: 1   Epoch: 13.56/20.0 (67.8% complete)   lr: 0.000210   shrink: 0.99790\n",
      "2020-11-02 17:35:58,067 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 123/179   Jobs: 1   Epoch: 13.67/20.0 (68.3% complete)   lr: 0.000207   shrink: 0.99793\n",
      "2020-11-02 17:36:08,568 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 124/179   Jobs: 1   Epoch: 13.78/20.0 (68.9% complete)   lr: 0.000205   shrink: 0.99795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 17:36:17,484 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 125/179   Jobs: 1   Epoch: 13.89/20.0 (69.4% complete)   lr: 0.000202   shrink: 0.99798\n",
      "2020-11-02 17:36:27,962 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 126/179   Jobs: 1   Epoch: 14.00/20.0 (70.0% complete)   lr: 0.000200   shrink: 0.99800\n",
      "2020-11-02 17:36:34,732 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 127/179   Jobs: 1   Epoch: 14.11/20.0 (70.6% complete)   lr: 0.000197   shrink: 0.99803\n",
      "2020-11-02 17:36:42,578 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 128/179   Jobs: 1   Epoch: 14.22/20.0 (71.1% complete)   lr: 0.000194   shrink: 0.99806\n",
      "2020-11-02 17:36:49,128 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 129/179   Jobs: 1   Epoch: 14.33/20.0 (71.7% complete)   lr: 0.000192   shrink: 0.99808\n",
      "2020-11-02 17:36:55,725 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 130/179   Jobs: 1   Epoch: 14.44/20.0 (72.2% complete)   lr: 0.000190   shrink: 0.99810\n",
      "2020-11-02 17:37:04,273 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 131/179   Jobs: 1   Epoch: 14.56/20.0 (72.8% complete)   lr: 0.000187   shrink: 0.99813\n",
      "2020-11-02 17:37:11,796 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 132/179   Jobs: 1   Epoch: 14.67/20.0 (73.3% complete)   lr: 0.000185   shrink: 0.99815\n",
      "2020-11-02 17:37:22,303 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 133/179   Jobs: 1   Epoch: 14.78/20.0 (73.9% complete)   lr: 0.000182   shrink: 0.99818\n",
      "2020-11-02 17:37:31,182 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 134/179   Jobs: 1   Epoch: 14.89/20.0 (74.4% complete)   lr: 0.000180   shrink: 0.99820\n",
      "2020-11-02 17:37:41,701 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 135/179   Jobs: 1   Epoch: 15.00/20.0 (75.0% complete)   lr: 0.000178   shrink: 0.99822\n",
      "2020-11-02 17:37:48,468 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 136/179   Jobs: 1   Epoch: 15.11/20.0 (75.6% complete)   lr: 0.000176   shrink: 0.99824\n",
      "2020-11-02 17:37:56,273 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 137/179   Jobs: 1   Epoch: 15.22/20.0 (76.1% complete)   lr: 0.000173   shrink: 0.99827\n",
      "2020-11-02 17:38:02,760 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 138/179   Jobs: 1   Epoch: 15.33/20.0 (76.7% complete)   lr: 0.000171   shrink: 0.99829\n",
      "2020-11-02 17:38:09,320 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 139/179   Jobs: 1   Epoch: 15.44/20.0 (77.2% complete)   lr: 0.000169   shrink: 0.99831\n",
      "2020-11-02 17:38:17,814 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 140/179   Jobs: 1   Epoch: 15.56/20.0 (77.8% complete)   lr: 0.000167   shrink: 0.99833\n",
      "2020-11-02 17:38:26,057 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 141/179   Jobs: 1   Epoch: 15.67/20.0 (78.3% complete)   lr: 0.000165   shrink: 0.99835\n",
      "2020-11-02 17:38:36,521 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 142/179   Jobs: 1   Epoch: 15.78/20.0 (78.9% complete)   lr: 0.000163   shrink: 0.99837\n",
      "2020-11-02 17:38:45,416 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 143/179   Jobs: 1   Epoch: 15.89/20.0 (79.4% complete)   lr: 0.000161   shrink: 0.99839\n",
      "2020-11-02 17:38:55,916 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 144/179   Jobs: 1   Epoch: 16.00/20.0 (80.0% complete)   lr: 0.000158   shrink: 0.99842\n",
      "2020-11-02 17:39:02,718 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 145/179   Jobs: 1   Epoch: 16.11/20.0 (80.6% complete)   lr: 0.000156   shrink: 0.99844\n",
      "2020-11-02 17:39:10,559 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 146/179   Jobs: 1   Epoch: 16.22/20.0 (81.1% complete)   lr: 0.000154   shrink: 0.99846\n",
      "2020-11-02 17:39:17,122 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 147/179   Jobs: 1   Epoch: 16.33/20.0 (81.7% complete)   lr: 0.000153   shrink: 0.99847\n",
      "2020-11-02 17:39:23,680 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 148/179   Jobs: 1   Epoch: 16.44/20.0 (82.2% complete)   lr: 0.000151   shrink: 0.99849\n",
      "2020-11-02 17:39:32,196 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 149/179   Jobs: 1   Epoch: 16.56/20.0 (82.8% complete)   lr: 0.000149   shrink: 0.99851\n",
      "2020-11-02 17:39:39,729 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 150/179   Jobs: 1   Epoch: 16.67/20.0 (83.3% complete)   lr: 0.000147   shrink: 0.99853\n",
      "2020-11-02 17:39:50,281 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 151/179   Jobs: 1   Epoch: 16.78/20.0 (83.9% complete)   lr: 0.000145   shrink: 0.99855\n",
      "2020-11-02 17:39:59,220 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 152/179   Jobs: 1   Epoch: 16.89/20.0 (84.4% complete)   lr: 0.000143   shrink: 0.99857\n",
      "2020-11-02 17:40:09,676 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 153/179   Jobs: 1   Epoch: 17.00/20.0 (85.0% complete)   lr: 0.000141   shrink: 0.99859\n",
      "2020-11-02 17:40:16,475 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 154/179   Jobs: 1   Epoch: 17.11/20.0 (85.6% complete)   lr: 0.000139   shrink: 0.99861\n",
      "2020-11-02 17:40:24,277 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 155/179   Jobs: 1   Epoch: 17.22/20.0 (86.1% complete)   lr: 0.000138   shrink: 0.99862\n",
      "2020-11-02 17:40:30,802 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 156/179   Jobs: 1   Epoch: 17.33/20.0 (86.7% complete)   lr: 0.000136   shrink: 0.99864\n",
      "2020-11-02 17:40:37,416 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 157/179   Jobs: 1   Epoch: 17.44/20.0 (87.2% complete)   lr: 0.000134   shrink: 0.99866\n",
      "2020-11-02 17:40:45,872 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 158/179   Jobs: 1   Epoch: 17.56/20.0 (87.8% complete)   lr: 0.000133   shrink: 0.99867\n",
      "2020-11-02 17:40:53,388 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 159/179   Jobs: 1   Epoch: 17.67/20.0 (88.3% complete)   lr: 0.000131   shrink: 0.99869\n",
      "2020-11-02 17:41:03,851 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 160/179   Jobs: 1   Epoch: 17.78/20.0 (88.9% complete)   lr: 0.000129   shrink: 0.99871\n",
      "2020-11-02 17:41:13,457 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 161/179   Jobs: 1   Epoch: 17.89/20.0 (89.4% complete)   lr: 0.000128   shrink: 0.99872\n",
      "2020-11-02 17:41:23,938 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 162/179   Jobs: 1   Epoch: 18.00/20.0 (90.0% complete)   lr: 0.000126   shrink: 0.99874\n",
      "2020-11-02 17:41:30,713 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 163/179   Jobs: 1   Epoch: 18.11/20.0 (90.6% complete)   lr: 0.000124   shrink: 0.99876\n",
      "2020-11-02 17:41:38,542 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 164/179   Jobs: 1   Epoch: 18.22/20.0 (91.1% complete)   lr: 0.000123   shrink: 0.99877\n",
      "2020-11-02 17:41:45,101 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 165/179   Jobs: 1   Epoch: 18.33/20.0 (91.7% complete)   lr: 0.000121   shrink: 0.99879\n",
      "2020-11-02 17:41:51,663 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 166/179   Jobs: 1   Epoch: 18.44/20.0 (92.2% complete)   lr: 0.000120   shrink: 0.99880\n",
      "2020-11-02 17:42:00,138 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 167/179   Jobs: 1   Epoch: 18.56/20.0 (92.8% complete)   lr: 0.000118   shrink: 0.99882\n",
      "2020-11-02 17:42:07,643 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 168/179   Jobs: 1   Epoch: 18.67/20.0 (93.3% complete)   lr: 0.000117   shrink: 0.99883\n",
      "2020-11-02 17:42:18,106 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 169/179   Jobs: 1   Epoch: 18.78/20.0 (93.9% complete)   lr: 0.000115   shrink: 0.99885\n",
      "2020-11-02 17:42:27,027 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 170/179   Jobs: 1   Epoch: 18.89/20.0 (94.4% complete)   lr: 0.000114   shrink: 0.99886\n",
      "2020-11-02 17:42:37,521 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 171/179   Jobs: 1   Epoch: 19.00/20.0 (95.0% complete)   lr: 0.000112   shrink: 0.99888\n",
      "2020-11-02 17:42:44,278 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 172/179   Jobs: 1   Epoch: 19.11/20.0 (95.6% complete)   lr: 0.000111   shrink: 0.99889\n",
      "2020-11-02 17:42:52,106 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 173/179   Jobs: 1   Epoch: 19.22/20.0 (96.1% complete)   lr: 0.000109   shrink: 0.99891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 17:42:58,638 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 174/179   Jobs: 1   Epoch: 19.33/20.0 (96.7% complete)   lr: 0.000108   shrink: 0.99892\n",
      "2020-11-02 17:43:05,248 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 175/179   Jobs: 1   Epoch: 19.44/20.0 (97.2% complete)   lr: 0.000107   shrink: 0.99893\n",
      "2020-11-02 17:43:13,701 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 176/179   Jobs: 1   Epoch: 19.56/20.0 (97.8% complete)   lr: 0.000105   shrink: 0.99895\n",
      "2020-11-02 17:43:21,213 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 177/179   Jobs: 1   Epoch: 19.67/20.0 (98.3% complete)   lr: 0.000104   shrink: 0.99896\n",
      "2020-11-02 17:43:31,709 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 178/179   Jobs: 1   Epoch: 19.78/20.0 (98.9% complete)   lr: 0.000103   shrink: 0.99897\n",
      "2020-11-02 17:43:40,594 [steps/nnet3/train_raw_dnn.py:391 - train - INFO ] Iter: 179/179   Jobs: 1   Epoch: 19.89/20.0 (99.4% complete)   lr: 0.000100   shrink: 0.99900\n",
      "2020-11-02 17:43:51,060 [steps/nnet3/train_raw_dnn.py:443 - train - INFO ] Doing final combination to produce final.raw\n",
      "2020-11-02 17:43:51,060 [steps/libs/nnet3/train/frame_level_objf/common.py:491 - combine_models - INFO ] Combining {161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180} models.\n",
      "2020-11-02 17:44:31,915 [steps/nnet3/train_raw_dnn.py:465 - train - INFO ] Cleaning up the experiment directory exp/xvector_nnet_1a\n",
      "exp/xvector_nnet_1a: num-iters=180 nj=3..1 num-params=4.6M dim=23->340 combine=-0.44->-0.43 (over 20) loglike:train/valid[119,179]=(-0.34,-0.34/-0.54,-0.56) accuracy:train/valid[119,179]=(0.927,0.927/0.899,0.876)\n"
     ]
    }
   ],
   "source": [
    "!chmod +x new_06_train_x-vector.sh\n",
    "!bash new_06_train_x-vector.sh --stage 0 --train-stage 0 --data data/train_combined_no_sil --nnet-dir exp/xvector_nnet_1a --egs-dir exp/xvector_nnet_1a/egs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mexp/xvector_nnet_1a/\u001b[00m\r\n",
      "├── 0.raw\r\n",
      "├── 4.raw\r\n",
      "├── accuracy.output.report\r\n",
      "├── cache.4\r\n",
      "├── \u001b[01;34mconfigs\u001b[00m\r\n",
      "│   ├── final.config\r\n",
      "│   ├── network.xconfig\r\n",
      "│   ├── ref.config\r\n",
      "│   ├── ref.raw\r\n",
      "│   ├── vars\r\n",
      "│   ├── xconfig\r\n",
      "│   ├── xconfig.expanded.1\r\n",
      "│   └── xconfig.expanded.2\r\n",
      "├── \u001b[01;34megs\u001b[00m\r\n",
      "│   ├── \u001b[01;36mcombine.scp\u001b[00m -> train_diagnostic.scp\r\n",
      "│   ├── egs.1.ark\r\n",
      "│   ├── egs.1.scp\r\n",
      "│   ├── egs.2.ark\r\n",
      "│   ├── egs.2.scp\r\n",
      "│   ├── egs.3.ark\r\n",
      "│   ├── egs.3.scp\r\n",
      "│   ├── egs.4.ark\r\n",
      "│   ├── egs.4.scp\r\n",
      "│   ├── egs.5.ark\r\n",
      "│   ├── egs.5.scp\r\n",
      "│   ├── egs.6.ark\r\n",
      "│   ├── egs.6.scp\r\n",
      "│   ├── egs.7.ark\r\n",
      "│   ├── egs.7.scp\r\n",
      "│   ├── egs.8.ark\r\n",
      "│   ├── egs.8.scp\r\n",
      "│   ├── egs.9.ark\r\n",
      "│   ├── egs.9.scp\r\n",
      "│   ├── \u001b[01;34minfo\u001b[00m\r\n",
      "│   │   ├── feat_dim\r\n",
      "│   │   ├── frames_per_eg\r\n",
      "│   │   ├── left_context\r\n",
      "│   │   ├── num_archives\r\n",
      "│   │   ├── num_diagnostic_archives\r\n",
      "│   │   ├── num_frames\r\n",
      "│   │   └── right_context\r\n",
      "│   ├── \u001b[01;34mlog\u001b[00m\r\n",
      "│   │   ├── allocate_examples_train.log\r\n",
      "│   │   ├── allocate_examples_train_subset.log\r\n",
      "│   │   ├── allocate_examples_valid.log\r\n",
      "│   │   ├── shuffle.1.log\r\n",
      "│   │   ├── shuffle.2.log\r\n",
      "│   │   ├── shuffle.3.log\r\n",
      "│   │   ├── shuffle.4.log\r\n",
      "│   │   ├── shuffle.5.log\r\n",
      "│   │   ├── shuffle.6.log\r\n",
      "│   │   ├── shuffle.7.log\r\n",
      "│   │   ├── shuffle.8.log\r\n",
      "│   │   ├── shuffle.9.log\r\n",
      "│   │   ├── train_create_examples.1.log\r\n",
      "│   │   ├── train_create_examples.2.log\r\n",
      "│   │   ├── train_create_examples.3.log\r\n",
      "│   │   ├── train_create_examples.4.log\r\n",
      "│   │   ├── train_create_examples.5.log\r\n",
      "│   │   ├── train_create_examples.6.log\r\n",
      "│   │   ├── train_create_examples.7.log\r\n",
      "│   │   ├── train_create_examples.8.log\r\n",
      "│   │   ├── train_subset_create_examples.1.log\r\n",
      "│   │   ├── train_subset_shuffle.1.log\r\n",
      "│   │   ├── train_subset_shuffle.2.log\r\n",
      "│   │   ├── train_subset_shuffle.3.log\r\n",
      "│   │   ├── valid_create_examples.1.log\r\n",
      "│   │   ├── valid_shuffle.1.log\r\n",
      "│   │   ├── valid_shuffle.2.log\r\n",
      "│   │   └── valid_shuffle.3.log\r\n",
      "│   ├── pdf2num\r\n",
      "│   ├── \u001b[01;34mtemp\u001b[00m\r\n",
      "│   │   ├── archive_chunk_lengths\r\n",
      "│   │   ├── outputs.1\r\n",
      "│   │   ├── outputs.2\r\n",
      "│   │   ├── outputs.3\r\n",
      "│   │   ├── outputs.4\r\n",
      "│   │   ├── outputs.5\r\n",
      "│   │   ├── outputs.6\r\n",
      "│   │   ├── outputs.7\r\n",
      "│   │   ├── outputs.8\r\n",
      "│   │   ├── ranges.1\r\n",
      "│   │   ├── ranges.2\r\n",
      "│   │   ├── ranges.3\r\n",
      "│   │   ├── ranges.4\r\n",
      "│   │   ├── ranges.5\r\n",
      "│   │   ├── ranges.6\r\n",
      "│   │   ├── ranges.7\r\n",
      "│   │   ├── ranges.8\r\n",
      "│   │   ├── spk2int\r\n",
      "│   │   ├── train_subset_archive_chunk_lengths\r\n",
      "│   │   ├── train_subset_outputs.1\r\n",
      "│   │   ├── train_subset_ranges.1\r\n",
      "│   │   ├── train_subset_uttlist\r\n",
      "│   │   ├── utt2int\r\n",
      "│   │   ├── utt2int.train\r\n",
      "│   │   ├── utt2int.train_subset\r\n",
      "│   │   ├── utt2int.valid\r\n",
      "│   │   ├── utt2num_frames\r\n",
      "│   │   ├── utt2num_frames.train\r\n",
      "│   │   ├── utt2num_frames.train_subset\r\n",
      "│   │   ├── utt2num_frames.valid\r\n",
      "│   │   ├── valid_archive_chunk_lengths\r\n",
      "│   │   ├── valid_outputs.1\r\n",
      "│   │   ├── valid_ranges.1\r\n",
      "│   │   └── valid_uttlist\r\n",
      "│   ├── train_diagnostic_egs.1.ark\r\n",
      "│   ├── train_diagnostic_egs.1.scp\r\n",
      "│   ├── train_diagnostic_egs.2.ark\r\n",
      "│   ├── train_diagnostic_egs.2.scp\r\n",
      "│   ├── train_diagnostic_egs.3.ark\r\n",
      "│   ├── train_diagnostic_egs.3.scp\r\n",
      "│   ├── train_diagnostic.scp\r\n",
      "│   ├── train_subset_pdf2num\r\n",
      "│   ├── valid_diagnostic.scp\r\n",
      "│   ├── valid_egs.1.ark\r\n",
      "│   ├── valid_egs.1.scp\r\n",
      "│   ├── valid_egs.2.ark\r\n",
      "│   ├── valid_egs.2.scp\r\n",
      "│   ├── valid_egs.3.ark\r\n",
      "│   ├── valid_egs.3.scp\r\n",
      "│   └── valid_pdf2num\r\n",
      "├── extract.config\r\n",
      "├── final.raw\r\n",
      "├── \u001b[01;34mlog\u001b[00m\r\n",
      "│   ├── average.1.log\r\n",
      "│   ├── average.2.log\r\n",
      "│   ├── average.3.log\r\n",
      "│   ├── combine.log\r\n",
      "│   ├── compute_prob_train.0.log\r\n",
      "│   ├── compute_prob_train.1.log\r\n",
      "│   ├── compute_prob_train.2.log\r\n",
      "│   ├── compute_prob_train.3.log\r\n",
      "│   ├── compute_prob_train.final.log\r\n",
      "│   ├── compute_prob_valid.0.log\r\n",
      "│   ├── compute_prob_valid.1.log\r\n",
      "│   ├── compute_prob_valid.2.log\r\n",
      "│   ├── compute_prob_valid.3.log\r\n",
      "│   ├── compute_prob_valid.final.log\r\n",
      "│   ├── progress.1.log\r\n",
      "│   ├── progress.2.log\r\n",
      "│   ├── progress.3.log\r\n",
      "│   ├── select.0.log\r\n",
      "│   ├── train.0.1.log\r\n",
      "│   ├── train.0.2.log\r\n",
      "│   ├── train.0.3.log\r\n",
      "│   ├── train.1.1.log\r\n",
      "│   ├── train.1.2.log\r\n",
      "│   ├── train.1.3.log\r\n",
      "│   ├── train.1.4.log\r\n",
      "│   ├── train.2.1.log\r\n",
      "│   ├── train.2.2.log\r\n",
      "│   ├── train.2.3.log\r\n",
      "│   ├── train.2.4.log\r\n",
      "│   ├── train.2.5.log\r\n",
      "│   ├── train.2.6.log\r\n",
      "│   ├── train.3.1.log\r\n",
      "│   ├── train.3.2.log\r\n",
      "│   ├── train.3.3.log\r\n",
      "│   ├── train.3.4.log\r\n",
      "│   ├── train.3.5.log\r\n",
      "│   ├── train.3.6.log\r\n",
      "│   └── train.3.7.log\r\n",
      "├── max_chunk_size\r\n",
      "├── min_chunk_size\r\n",
      "├── nnet.config\r\n",
      "└── srand\r\n",
      "\r\n",
      "6 directories, 161 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree exp/xvector_nnet_1a/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第七步：** 使用x-vector模型,提取embedding\n",
    "\n",
    "    1.（train_combined）利用训练好的模型，提取训练集的x-vector 输出位置：exp/xvectors_train_combined\n",
    "    2. (train_combined) 对数据进行LDA降维  输出位置：exp/xvectors_train_combined/transform.mat\n",
    "    3. (train_combined) 对数据进行PLDA建模 输出位置：exp/xvectors_train_combined/plda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x new_07_train.sh\n",
    "!bash new_07_train.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第八步：** 从测试集中划分训练集和测试集\n",
    "    \n",
    "    1.注意线程数量。\n",
    "    2.划分注册语音和测试语音\n",
    "    3.分别提取他们的x-vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils/fix_data_dir.sh: file data/test/enroll/utt2spk is not in sorted order or not unique, sorting it\n",
      "fix_data_dir.sh: kept all 60 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/test/enroll/.backup\n",
      "utils/fix_data_dir.sh: file data/test/eval/utt2spk is not in sorted order or not unique, sorting it\n",
      "fix_data_dir.sh: kept all 7116 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/test/eval/.backup\n",
      "sid/nnet3/xvector/extract_xvectors.sh --cmd run.pl --mem 10G --mem 4G --nj 1 --use-gpu true exp/xvector_nnet_1a data/test exp/xvector_nnet_1a/xvectors_test\n",
      "sid/nnet3/xvector/extract_xvectors.sh: using exp/xvector_nnet_1a/extract.config to extract xvectors\n",
      "sid/nnet3/xvector/extract_xvectors.sh: extracting xvectors for data/test\n",
      "sid/nnet3/xvector/extract_xvectors.sh: extracting xvectors from nnet\n",
      "sid/nnet3/xvector/extract_xvectors.sh: combining xvectors across jobs\n",
      "sid/nnet3/xvector/extract_xvectors.sh: computing mean of xvectors for each speaker\n"
     ]
    }
   ],
   "source": [
    "!chmod +x new_08_create_task.sh\n",
    "!bash new_08_create_task.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第九步：** 计算结果\n",
    "    \n",
    "    1.提取测试集的表征，因为测试集的数量比较少，使用40进程容易出错。\n",
    "    2.利用训练好的plda，以及测试集的x-vector生成每一对的得分。\n",
    "    3.计算EER，minDCF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 1.04%\r\n",
      "minDCF(p-target=0.01): 0.1338\r\n",
      "minDCF(p-target=0.001): 0.2268\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x new_09_result.sh\n",
    "!bash new_09_result.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
